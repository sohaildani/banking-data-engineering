ğŸ¦ Banking Data Engineering Platform (Senior Data Engineer)

End-to-end PySpark data engineering project inspired by real-world banking platforms (ENBD-style).
Designed to demonstrate senior-level architecture, data modeling, and Spark best practices â€” not toy examples.

ğŸ‘¤ Who this project is for

This repository is aimed at:

Senior / Lead Data Engineer roles

Banking, FinTech, Enterprise Data Platforms

Interview discussions around design, scalability, and governance

If youâ€™re a recruiter or hiring manager: this repo reflects how data pipelines are actually built and explained in banks.

ğŸš€ Project Summary

The platform simulates a banking analytics pipeline that processes:

Customer master data (KYC lifecycle)

Account balances

Financial transactions

Using a Medallion Architecture (Bronze â†’ Silver â†’ Gold), the system ensures:

Auditability

Historical tracking (SCD Type 2)

Clean separation of concerns

Re-runnable, idempotent jobs

ğŸ§± Architecture Overview
Raw Sources (CSV / Landing Zone)
        â”‚
        â–¼
ğŸŸ¤ Bronze Layer  â†’ Immutable raw ingestion (audit & replay)
        â”‚
        â–¼
âšª Silver Layer  â†’ Cleansed data + business logic
        â”‚              â€¢ Data quality checks
        â”‚              â€¢ SCD Type 2 (customer KYC history)
        â–¼
ğŸŸ¡ Gold Layer   â†’ Analytics-ready datasets & KPIs
        â”‚
        â–¼
BI / Reporting / ML
ğŸ§  Key Engineering Concepts Demonstrated
âœ… Medallion Architecture

Clear separation between raw, refined, and analytical layers â€” a standard pattern in large banks.

âœ… SCD Type 2 (Customer Dimension)

Tracks historical KYC status changes using effective dates and current flags.

âœ… Data Quality Enforcement

Explicit null checks and validation before promoting data downstream.

âœ… Spark Performance Awareness

Adaptive Query Execution enabled

Controlled shuffle partitions

Centralized Spark configuration

âœ… Idempotent & Re-runnable Pipelines

Jobs can be safely re-run without corrupting downstream layers.

ğŸ“ Repository Structure
banking-data-engineering/
â”œâ”€â”€ config/        # Central Spark configuration
â”œâ”€â”€ data/          # Sample raw input data
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ bronze/    # Raw ingestion layer
â”‚   â”œâ”€â”€ silver/    # Cleansing & business logic
â”‚   â”œâ”€â”€ gold/      # Analytics & KPIs
â”‚   â””â”€â”€ utils/     # Reusable Spark & data quality utilities
â””â”€â”€ scripts/       # Pipeline execution scripts
â–¶ How to Run (Local)
pip install -r requirements.txt
bash scripts/run_pipeline.sh

Each layer executes independently and can be monitored or re-run in isolation.

ğŸ—£ï¸ How to Discuss This in Interviews

Strong, senior-level talking points:

â€œWe keep Bronze immutable for audit and regulatory replay.â€

â€œCustomer KYC history is modeled using SCD Type 2.â€

â€œSilver applies data quality and business rules.â€

â€œGold is optimized for analytics and reporting, not transformations.â€

â€œSpark configs are centralized to avoid environment drift.â€

This project is intentionally structured to support design discussions, not just code walkthroughs.

ğŸ¦ Why This Matches Banking Environments

Regulatory-friendly design

Historical traceability

Clear data ownership per layer

Scalable Spark patterns used in production

This mirrors how pipelines are implemented in Emirates NBDâ€“scale data platforms.

ğŸ”® Possible Enhancements

Delta Lake + MERGE for true upserts

Airflow orchestration

Cloud storage (S3 / ADLS)

Schema evolution & governance

Fraud / AML analytical use cases

ğŸ‘‹ Author

Mohd Sohail Khan
Senior Data Engineer

ğŸ“Œ GitHub: https://github.com/sohaildani
